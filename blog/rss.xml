<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tom's Data Science Blog</title><link>https://tomwhi.github.com/</link><description>Tom's Data Science Blog</description><atom:link href="https://tomwhi.github.com/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2023 &lt;a href="mailto:thomaswhitington@gmail.com"&gt;Tom Whitington&lt;/a&gt; </copyright><lastBuildDate>Sun, 05 Feb 2023 15:56:26 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>More efficient labelling via a modified loss function</title><link>https://tomwhi.github.com/posts/multilabel-classification-modified-loss/</link><dc:creator>Tom Whitington</dc:creator><description>&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;One can dramatically reduce the cost of labelling data for a multi-label classifier, using a custom loss function adapted from binary cross entropy.&lt;/p&gt;
&lt;h2&gt;Labelling data is expensive&lt;/h2&gt;
&lt;p&gt;Supervised learning requires labelled data, and manually labelling examples - for example identifying categories for documents - can be expensive.&lt;/p&gt;
&lt;p&gt;In the Contextual team at Schibsted, we use natural language processing and machine learning to derive value from text data, such as news articles from Schibsted media brands, including Aftonbladet, Afterposten, SvD, and VG. One of our products is a system for matching news articles to contextual advertising campaigns that make the matches using news article content, rather than user browsing history.&lt;/p&gt;
&lt;p&gt;Brand safety is an important concern in contextual advertising: articles that are deemed brand unsafe for a given campaign should not be matched to it. We have investigated text classification as a way to ensure brand safety of contextual advertising campaigns. A challenge with this approach is that the subtantial cost of labelling when producing a training dataset for a multi-label text classifier.&lt;/p&gt;
&lt;p&gt;This blog post presents how a custom loss function can be used to substantially reduce the burden of data labelling.&lt;/p&gt;
&lt;h2&gt;Labelling for multi-label classification&lt;/h2&gt;
&lt;p&gt;When training a multi-label classifier with supervised learning, one typically starts with a dataset of &lt;em&gt;N&lt;sub&gt;total&lt;/sub&gt;&lt;/em&gt; examples, where each example includes the input item, together with a vector of &lt;em&gt;C&lt;/em&gt; labels, where &lt;em&gt;C&lt;/em&gt; is the number of categories. Producing such a dataset will require O(&lt;em&gt;N&lt;sub&gt;total&lt;/sub&gt; x C&lt;/em&gt;) time. Therefore, labelling data is particularly expensive for multi-label classification problems.&lt;/p&gt;
&lt;p&gt;Given a suitable training dataset, the standard approach is to use the &lt;a href="https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451"&gt;binary cross entropy loss function&lt;/a&gt; when training a multi-label classifier. In pytorch, this is implemented in &lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"&gt;&lt;code&gt;torch.nn.BCELoss&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"&gt;&lt;code&gt;torch.nn.BCEWithLogitsLoss&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;BCEWithLogitsLoss&lt;/code&gt; is the same as &lt;code&gt;torch.nn.BCELoss&lt;/code&gt; but with an initial sigmoid layer on the inputs, so that one can avoid numerical instability that can occur when working with (potentially tiny) probability values.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;BCEWithLogitsLoss&lt;/code&gt; function takes as input a batch of "logit" values (scores that can be converted to probabilities using the sigmoid function) - each with &lt;em&gt;N&lt;/em&gt; rows and &lt;em&gt;C&lt;/em&gt; columns - and corresponding labels, each label being 0 or 1 for the given example and category:
&lt;img alt="BCE loss inputs" src="https://tomwhi.github.com/images/loss_function/BCELossInputs.png"&gt;&lt;/p&gt;
&lt;p&gt;For a given item in the batch (&lt;em&gt;i.e.&lt;/em&gt; a single row from &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;), the loss is given by the following formula:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BCE loss single item" src="https://tomwhi.github.com/images/loss_function/BCELossFormulaSingleItem.png"&gt;&lt;/p&gt;
&lt;p&gt;Here, $\sigma$ is the element-wise logistic (sigmoid) function - so it is applied to each element of &lt;em&gt;x&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt;. Here's an example of this computation for the first item in the batch from above:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Loss example calculation" src="https://tomwhi.github.com/images/loss_function/LossExampleCalculation.png"&gt;&lt;/p&gt;
&lt;p&gt;The overall loss for a batch of data is then simply the mean of the loss scores for the individual items in the batch:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BCE mean calculation" src="https://tomwhi.github.com/images/loss_function/LossMeanCalculation.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Side-note: the negative log of a probability is known as the information content ("self information") of an event. So, l&lt;sub&gt;n&lt;/sub&gt; ends up being the sum of the information content contributed by each of the categories.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Leave out labels for input examples?&lt;/h2&gt;
&lt;p&gt;The binary cross entropy loss function requires each item to be labelled for every category. However, &lt;em&gt;&lt;strong&gt;it would be really nice&lt;/strong&gt;&lt;/em&gt; to be able to leave out labels - perhaps even only labelling a single category per input example. This would make it much easier to accrue positive and negative examples.&lt;/p&gt;
&lt;p&gt;There are at least two phases of data collection where such a technique could prove useful. When initially creating a training dataset, one could seek out candidate positive and negative examples for a given category, and then manually review them to confirm or flip the candidate label for each item. The technique could also prove useful when improving the dataset, as it would allow one to seek out particularly tricky postive and negative examples for a particular category, and to only have to label the given category for those examples.&lt;/p&gt;
&lt;p&gt;If we don't have a technique allowing us to leave out labels, then we would be forced to provide labels for all other categories, which can increase the labelling burden by a factor of &lt;em&gt;C&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A simple approach here would be to simply train &lt;em&gt;C&lt;/em&gt; separate binary classifiers, and to combine them into one classifier after they are each trained individually. However, this approach is somewhat inelegant, and could be wasteful in terms of memory and compute. Furthermore, model weights are not shared, which means that there is no possibility for data from one task to inform the classifier for another task.&lt;/p&gt;
&lt;h2&gt;Custom loss function&lt;/h2&gt;
&lt;p&gt;One solution to this problem is to modify the binary cross entropy loss function to ignore specified categories when computing loss for a given input example. This can be implemented by specifying a label "mask" for each input example, as discussed &lt;a href="https://github.com/keras-team/keras/issues/3893"&gt;here&lt;/a&gt; and &lt;a href="https://www.dlology.com/blog/how-to-multi-task-learning-with-missing-labels-in-keras/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here, we illustrate this approach, focusing on the case where only a single category has a label for each training example. Using this approach, the new loss function still accepts logits as the input &lt;em&gt;x&lt;/em&gt;, but the label matrix &lt;em&gt;y&lt;/em&gt; is modified so that only one category has a label for a given input item, and every other category has a null label of &lt;em&gt;-1&lt;/em&gt; for that input example.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Modified loss inputs" src="https://tomwhi.github.com/images/loss_function/ModifiedLossInputs.png"&gt;&lt;/p&gt;
&lt;p&gt;The loss for a single item in a batch is then modified to only look at the logit score and label for the non-null category:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Modified loss single item" src="https://tomwhi.github.com/images/loss_function/ModifiedLossFormulaSingleItem.png"&gt;&lt;/p&gt;
&lt;p&gt;Here's what that computation would look like for both items from the batch above:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Modified loss example calculation" src="https://tomwhi.github.com/images/loss_function/ModifiedLossExampleCalculation.png"&gt;&lt;/p&gt;
&lt;p&gt;Ideally, we'd like to be able to cope with the more general requirement of masking zero or more categories for each input example, even though we may in practice only label one category for each example. The following code snippet implements this more general masking approach in pytorch:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BCEOnSelectedLogitLoss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_Loss&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduction&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'mean'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BCEOnSelectedLogitLoss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduction&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Extract a mask matrix from the labels matrix:&lt;/span&gt;
        &lt;span class="n"&gt;mask_symbol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;mask_symbol&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Eliminate the mask value (-1) from the labels to avoid numerical problems&lt;/span&gt;
        &lt;span class="c1"&gt;# when inputting it to the original loss function:&lt;/span&gt;
        &lt;span class="n"&gt;labels_no_mask_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;

        &lt;span class="c1"&gt;# Get the individual loss function contributions for each category and example:&lt;/span&gt;
        &lt;span class="n"&gt;loss_function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BCEWithLogitsLoss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss_without_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels_no_mask_values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Convert contributions to zero as indicated by the mask values:&lt;/span&gt;
        &lt;span class="n"&gt;loss_with_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss_without_mask&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;

        &lt;span class="c1"&gt;# Each unmasked category example will contribute equally to the final loss:&lt;/span&gt;
        &lt;span class="n"&gt;loss_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss_with_mask&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss_mean&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A quick sanity check confirms that the loss function produces the same result when we run the original loss function on only the selected categories:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;bce_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BCEWithLogitsLoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;logits_matrix1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.5&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;labels_matrix1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bce_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits_matrix1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels_matrix1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;selected_bce_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BCEOnSelectedLogitLoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;logits_matrix2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;4.5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;5.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;6.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;7.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.5&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;labels_matrix2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;selected_bce_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits_matrix2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels_matrix2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;tensor(5.5395, dtype=torch.float64)
tensor(5.5395, dtype=torch.float64)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We gave this modified loss function a spin on an inhouse brand safety classfication dataset, with our model consisting of a simple embedding layer followed by a linear layer (adapted from the pytorch &lt;a href="https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"&gt;text classification tutorial&lt;/a&gt;, and similar to the &lt;a href="https://fasttext.cc/docs/en/supervised-tutorial.html"&gt;FastText&lt;/a&gt; architecture). We confirmed that performance is similar using &lt;em&gt;C&lt;/em&gt; separate binary FastText classifiers, as judged by AUC for the individual classifiers.&lt;/p&gt;
&lt;p&gt;So, this does indeed seem to be a viable approach for training a single multi-label classifier from a dataset where each item only includes a label for a single category: it works! ð¥³&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Results are not shown - out of scope for this blogpost.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Using a custom loss function based on binary cross entropy, one can include selectively labelled examples, without having to label every category. This can reduce the labelling cost by a factor of &lt;em&gt;C = Number of categories&lt;/em&gt;, when one only wishes to label a single category for each example. We applied this approach in the context of multilabel text classification, but it is equally applicable to other modalities, such as image data.&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Social media:&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;If you would like to get in touch, here's my social media:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LinkedIn: &lt;a href="https://www.linkedin.com/in/tomwhitington"&gt;https://www.linkedin.com/in/tomwhitington&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Twitter: &lt;a href="https://twitter.com/Tom_Whitington"&gt;@Tom_Whitington&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mastodon: @tomwhitington@sigmoid.social&lt;/li&gt;
&lt;/ul&gt;</description><guid>https://tomwhi.github.com/posts/multilabel-classification-modified-loss/</guid><pubDate>Fri, 03 Feb 2023 10:22:00 GMT</pubDate></item><item><title>Using word vectors to decipher Swedish culture</title><link>https://tomwhi.github.com/posts/using-word-vectors-to-decipher-swedish-culture/</link><dc:creator>Tom Whitington</dc:creator><description>&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;Can culture be quantified? Taking for example a statement like:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;"Society has become more liberal over the years."&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;em&gt;"In Sweden it is particularly important not to brag."&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How do we know whether these statements are accurate?&lt;/p&gt;
&lt;p&gt;Such questions relating to culture often prove refractory to quantitative analysis. However, recent developments in natural language processing (NLP) are providing new avenues of investigation. For example, the development of high quality &lt;a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank"&gt;word embeddings&lt;/a&gt; provides a mechanism for quantifying the meaning of words, as derived from input text corpora.&lt;/p&gt;
&lt;p&gt;Here, I present an attempt to use word vectors to analyse culture. In particular, I have analysed &lt;a href="https://arxiv.org/abs/1301.3781" target="_blank"&gt;word2vec&lt;/a&gt; word embeddings trained on English and Swedish wikipedia corpuses, to examine whether there are particular areas of expression that are enriched or depleted in one language compared to another.&lt;/p&gt;
&lt;p&gt;Below, I explain the analysis. To skip straight to the nice meaty results, &lt;a href="https://tomwhi.github.com/posts/using-word-vectors-to-decipher-swedish-culture/#results"&gt;click here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="clusters aesthetic" src="https://tomwhi.github.com/images/WordVectors/All_Clusters_Aesthetic_v3.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Word2vec and machine translation&lt;/h2&gt;
&lt;p&gt;Word2vec is very cool indeed. The method produces high dimensional word embeddings by training a neural network to predict words given their context, from an input text corpus. The resulting &lt;a href="https://www.tensorflow.org/tutorials/word2vec/" target="_blank"&gt;word vectors&lt;/a&gt; have interesting semantic properties. To take a famous example, if we take the vector for the word "King", subtract the vector for "man" and add the vector for "woman", we end up with a vector located close to the word "Queen".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1309.4168" target="_blank"&gt;Mikolov &lt;em&gt;et al.&lt;/em&gt;&lt;/a&gt; also found that the relative positioning of words in one language are preserved to some extent when taking their translations in a second language. The authors showed how this can facilitate machine translation of words: A &lt;a href="https://en.wikipedia.org/wiki/Transformation_matrix" target="_blank"&gt;transformation matrix&lt;/a&gt; can be trained, such that multiplication of a word vector in language &lt;em&gt;l&lt;/em&gt;&lt;sub&gt;query&lt;/sub&gt; will result in a vector that is close (on average) to a suitable translation in language &lt;em&gt;l&lt;/em&gt;&lt;sub&gt;target&lt;/sub&gt;.&lt;/p&gt;
&lt;p&gt;This leads to my project. One could apply Mikolov's method to all words in language &lt;em&gt;l&lt;/em&gt;&lt;sub&gt;query&lt;/sub&gt; such that they are comparable to words in language &lt;em&gt;l&lt;/em&gt;&lt;sub&gt;target&lt;/sub&gt;. This would result in two word landscapes in high-dimensional space, which can themselves be compared for various properties. For example, one language may be enriched or depleted for specific areas of expression relative to another. I implemented this approach and used it to compare English and Swedish, with the aim of identifying interesting cultural differences.&lt;/p&gt;
&lt;h2&gt;Implementing Mikolov &lt;em&gt;et al.&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;The code I wrote for this project is available on &lt;a href="https://github.com/tomwhi/nlp-stuff" target="_blank"&gt;GitHub&lt;/a&gt;. I implemented the project as a series of small python scripts, which cobble together to form a &lt;a href="https://github.com/tomwhi/nlp-stuff/blob/master/example_pipeline.sh" target="_blank"&gt;rough analysis pipeline&lt;/a&gt;. I focused on completing the project, rather than on software engineering &lt;em&gt;per se&lt;/em&gt;, so some of it is a bit rough and ready.&lt;/p&gt;
&lt;p&gt;My work relies on the &lt;a href="https://radimrehurek.com/gensim/" target="_blank"&gt;gensim&lt;/a&gt; library, which includes amongst other things an implementation of the word2vec training algorithm. I found the library extremely intuitive and powerful.&lt;/p&gt;
&lt;p&gt;I produced word vectors and a transformation matrix through the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tomwhi/nlp-stuff/blob/master/process_wiki.py" target="_blank"&gt;Processing&lt;/a&gt; the wikipedia corpus for input to gensim, for both English and Swedish.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tomwhi/nlp-stuff/blob/master/train_word2vec.py" target="_blank"&gt;Training&lt;/a&gt; word2vec models using gensim, including short phrases in the vocabulary in addition to individual words.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tomwhi/nlp-stuff/blob/master/filter_word2vec_on_vocab.py" target="_blank"&gt;Filtering&lt;/a&gt; the resulting word vectors to only retain words in predefined English and Swedish vocabularies&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tomwhi/nlp-stuff/blob/master/run_microsoft_translation.py" target="_blank"&gt;Obtaining translations&lt;/a&gt; for the most frequent words using the Microsoft translation API, to use for training the transformation matrix, and &lt;a href="https://github.com/tomwhi/nlp-stuff/blob/master/retrieve_training_vectors.py"&gt;retrieving&lt;/a&gt; corresponding word vector pairs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tomwhi/nlp-stuff/blob/master/derive_translation_matrix.py" target="_blank"&gt;Training&lt;/a&gt; the transformation matrix, by implementing gradient descent with the loss function defined in &lt;a href="https://arxiv.org/abs/1309.4168" target="_blank"&gt;Mikolov &lt;em&gt;et al.&lt;/em&gt;&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="equation" src="https://tomwhi.github.com/images/WordVectors/Equation.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Here, &lt;em&gt;W&lt;/em&gt; is the translation matrix, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;i&lt;/sub&gt; is the &lt;em&gt;i&lt;/em&gt; th training word vector in the query language and &lt;em&gt;z&lt;/em&gt;&lt;sub&gt;i&lt;/sub&gt; is the word vector for the corresponding translation. I used &lt;a href="https://github.com/Theano/Theano" target="_blank"&gt;Theano&lt;/a&gt; to implement the gradient descent in this step, and manually checked the partial derivatives on a small example matrix to make sure I got the same results as Theano (having not used Theano prior to this). I plotted the cost function with increasing training iterations in order to see how different training rates impacted the effectiveness of the gradient descent.&lt;/li&gt;
&lt;li&gt;I then applied the transformation matrix to all Swedish word vectors to obtain corresponding vectors that are then comparable to the English word vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inspecting some example words and their translations indicates that the translation works quite well, as illustrated by the shift in Swedish word vectors between Figure 1 and Figure 2:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: Scatterplot showing a selection of English words (red) and their corresponding Swedish words (blue), connected by light grey lines, when the word vectors are projected onto the first two principal components derived from running &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank"&gt;PCA&lt;/a&gt; on the English word vectors:&lt;/em&gt;
&lt;img alt="PC plot 1" src="https://tomwhi.github.com/images/WordVectors/PC_Plot_English_Swedish.png"&gt;
&lt;sub&gt;&lt;sup&gt;&lt;em&gt;&lt;strong&gt;Technical note&lt;/strong&gt;: Swedish word vectors can be projected onto the English principal components 1 and 2, as the Swedish and English word vectors just happen to be the same length (400 elements). This is done in figure 1 simply to contrast against their updated positions as shown in figure 2, after the translation matrix is applied.&lt;/em&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: When I multiply the Swedish word vectors with the translation matrix, the word vectors move much close to their respective English counterparts:&lt;/em&gt;
&lt;img alt="PC plot 2" src="https://tomwhi.github.com/images/WordVectors/PC_Plot_English_Swedish_Translated.png"&gt;&lt;/p&gt;
&lt;h2&gt;Mitigating the "curse of dimensionality"&lt;/h2&gt;
&lt;p&gt;Given word vectors for English and Swedish (translated to English word vector coordinates), I now set out to compare the languages based on the positioning of the word vectors in high dimensional space. This was by far and away the trickiest and most time-consuming aspect of the project, and I ended up trying a few different approaches to the problem.&lt;/p&gt;
&lt;p&gt;To get optimal performance, word vectors are high dimensional (typically hundreds of dimensions). The high dimensionality of the resulting data can cause various &lt;a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank"&gt;problems&lt;/a&gt;. In this project, data sparsity was a particular problem: I needed to find a way to compare the English and Swedish word vector landscapes in spite of the great sparsity of the word vector instances. Given 400 dimensions, a volume in that space will typically contain few word vectors.&lt;/p&gt;
&lt;p&gt;I tried applying &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank"&gt;PCA&lt;/a&gt; and doing a comparison of word density in volumes defined by the resulting lower number of dimensions, and I tried out the &lt;a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank"&gt;t-SNE&lt;/a&gt; method too.&lt;/p&gt;
&lt;p&gt;My goal was to identify areas of linguistic expression enriched in one language relative to another, and so I eventually decided I should define clusters of words within similar meaning, and then analyse those on aggregate. To do this, I used gensim to find the &lt;a href="https://github.com/tomwhi/nlp-stuff/blob/master/get_closest_words.py" target="_blank"&gt;closest&lt;/a&gt; 100 English word vectors for each English word, as defined by &lt;a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank"&gt;cosine similarity&lt;/a&gt;. I then defined a &lt;a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics" target="_blank"&gt;graph&lt;/a&gt;) of word similarity, with words as nodes and edges between nodes if the two words have cosine similarity &amp;gt; 0.5. Taking this graph as input, I ran the &lt;a href="http://www.mapequation.org/code.html" target="_blank"&gt;InfoMap tool&lt;/a&gt; to detect clusters of highly interconnected words. For each of the word clusters, I also calculated the median cosine similarity value to the closest Swedish word, considering all words in the cluster.&lt;/p&gt;
&lt;p&gt;Using this approach produced more robust results compared with an approach analysing individual words in isolation.&lt;/p&gt;
&lt;h2&gt;Highly translatable words&lt;/h2&gt;
&lt;p&gt;I consider the median Swedish cosine similarity to be a proxy for the &lt;em&gt;translatability&lt;/em&gt; of a given word cluster - &lt;em&gt;i.e.&lt;/em&gt; word clusters with a high score contain words that typically have a good translation, whilst word clusters with a low score contain words with mostly poor translations.&lt;/p&gt;
&lt;p&gt;Looking at the clusters with highest translatability, we can see that the clusters deal with universal concepts that are not tethered to culture, including numbers, physical positioning, time, or physical actions, as shown in Figure 3.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: Clusters of similar English words (red nodes) with high median cosine similarity of their closest Swedish translations (blue nodes), defined through use of InfoMap. Visualisation generated with &lt;a href="https://gephi.org" target="_blank"&gt;Gephi&lt;/a&gt;. Edges between English word nodes indicate cosine similarity &amp;gt; 0.5, whereas an edge to a Swedish word node indicates the translation with highest cosine similarity for the given English word. The size of Swedish word nodes is scaled to the maximum observed cosine similarity score for that word. Note: The numeric values associated with the Swedish words are a technical artefact, and can be ignored.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="NegCntrlClusters" src="https://tomwhi.github.com/images/WordVectors/NegCntrl_Clusters.png"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a name="results"&gt;&lt;/a&gt;Cultural insights: The Jantelagen and the Swagman&lt;/h2&gt;
&lt;p&gt;Some of the clusters with low translatability scores reveal potentially cultural differences between Swedish- and English-speaking populations. I considered the clusters in the lowest 10% of translatability (72 clusters in total), and present illustrative examples here (more extensive results are presented in the &lt;a href="https://tomwhi.github.com/posts/using-word-vectors-to-decipher-swedish-culture/#appendix"&gt;appendix&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;One unspoken rule underpinning Scandinavian societies is the &lt;a href="https://en.wikipedia.org/wiki/Law_of_Jante" target="_blank"&gt;"Jantelagen"&lt;/a&gt;. Under the Jantelagen, it is taboo to promote oneself as having greater merit or achievement compared to others. As such, it is perhaps no surprise that English words such as &lt;em&gt;eclipsed&lt;/em&gt;, &lt;em&gt;surpassed&lt;/em&gt;, &lt;em&gt;rivaled&lt;/em&gt;, &lt;em&gt;bettered&lt;/em&gt; and &lt;em&gt;outpaced&lt;/em&gt; are difficult to translate to Swedish equivalents (Figure 4).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 4: English words that appear to violate the Swedish Jantelagen (see Figure 3 caption for legend).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Jantelagen cluster" src="https://tomwhi.github.com/images/WordVectors/Jantelagen_cluster_v1.png"&gt;&lt;/p&gt;
&lt;p&gt;Scandinavian societies are also famous for being highly egalitarian - a concept extending beyond the Jantelagen itself. My method identifies several relevant clusters of English words that are indeed not very egalitarian in tone. In Figure 5 we can see a cluster of words containing various occupations. Some of these arguably exist solely for rich people to flaunt their wealth - such as &lt;em&gt;butlers&lt;/em&gt; or &lt;em&gt;valets&lt;/em&gt;, and others might be considered old fashioned, such as &lt;em&gt;housekeepers&lt;/em&gt; or &lt;em&gt;homemakers&lt;/em&gt;. In a similar vein, words like &lt;em&gt;profiteering&lt;/em&gt;, &lt;em&gt;mongering&lt;/em&gt;, &lt;em&gt;debased&lt;/em&gt;, and &lt;em&gt;bullish&lt;/em&gt; seem to run counter to ideals of equality.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 5: English words and concepts that run counter to egalitarianism.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Egalitarianism cluster" src="https://tomwhi.github.com/images/WordVectors/Egalitarianism_cluster_v1.png"&gt;&lt;/p&gt;
&lt;p&gt;Figure 6 shows a cluster of morality-related verbs such as &lt;em&gt;sinned&lt;/em&gt; and &lt;em&gt;transgressed&lt;/em&gt;, and a cluster of nouns/adjectives relating to virtues, including &lt;em&gt;valour&lt;/em&gt; and &lt;em&gt;gallantry&lt;/em&gt;. These are concepts that vary from culture to culture; such ideas could be considered pompous or pious depending on your point of view.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 6: English words relating to virtue concepts that translate poorly to Swedish.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pomposity cluster" src="https://tomwhi.github.com/images/WordVectors/Pomposity_clusters_v1.png"&gt;&lt;/p&gt;
&lt;p&gt;Various English word clusters relating to north american sporting terminology (baseball, gridiron football) as well as famous computer games (&lt;em&gt;ultima&lt;/em&gt;, &lt;em&gt;resident evil&lt;/em&gt;) also translate poorly into Swedish (Figure 7). This is clearly an expected result, as Swedes simply revert to using the English terminology when discussing such topics.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 7: Sporting/gaming terms that translate poorly to Swedish.&lt;/em&gt;
&lt;img alt="Sports cluster" src="https://tomwhi.github.com/images/WordVectors/Sports_cluster_v1.png"&gt;&lt;/p&gt;
&lt;p&gt;Finally, as an Australian, here is my favourite result of all:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 8: Archaic professions of the Australian bush&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Swagman cluster" src="https://tomwhi.github.com/images/WordVectors/Swagman_cluster_v1.png"&gt;&lt;/p&gt;
&lt;p&gt;Of course, most Swedes will have not the foggiest of what a &lt;em&gt;bushranger&lt;/em&gt; or a &lt;em&gt;swagman&lt;/em&gt; is. These words are all professions in the Australian outback, in colonial times. A &lt;a href="https://en.wikipedia.org/wiki/Swagman" target="_blank"&gt;swagman&lt;/a&gt; was someone a bit down on their luck, travelling around the Australian bush looking for work here and there (Figure 9).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 9: A swagman&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Swagman" src="https://tomwhi.github.com/images/WordVectors/swagman.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The method gets fairly close for "bushranger", coming up with the Swedish word "pirat" (which is equivelant to the English word "pirate"). &lt;a href="https://en.wikipedia.org/wiki/Bushranger" target="_blank"&gt;Bushrangers&lt;/a&gt; were people hiding in the bush to evade the authorities, occasionally fighting the police (Figure 10).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 10: No, Swedes, that's not a SÃ¶dermalm hipster. It's Ned Kelly, Australia's most famous bushranger, with his home-made suit of armour!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ned Kelly" src="https://tomwhi.github.com/images/WordVectors/Ned_Kelly_in_1880.png"&gt;&lt;/p&gt;
&lt;h2&gt;Perspectives&lt;/h2&gt;
&lt;p&gt;If someone asked me,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;"What did you find, in your quest for the word vectors?"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I would answer:&lt;/p&gt;
&lt;p&gt;&lt;img alt="illumination" src="https://tomwhi.github.com/illumination_subtitled.gif"&gt;&lt;/p&gt;
&lt;p&gt;The project was a lot of fun and I have learnt some new skills whilst doing it, including how to analyse and visualise word vectors, and implementing the gradient descent using Theano.&lt;/p&gt;
&lt;p&gt;I am reasonably confident in the veracity of my findings, with some caveats (see &lt;a href="https://tomwhi.github.com/posts/using-word-vectors-to-decipher-swedish-culture/#appendix"&gt;appendix&lt;/a&gt;). On the whole, this approach seems to turn up some genuine areas of linguistic expression that are enriched in one language relative to another (in this case English vs Swedish). By inspecting the sets of words enriched in English relative to Swedish, the method seems to produce insights into cultural differences between the English- and Swedish-speaking communities.&lt;/p&gt;
&lt;p&gt;Rigorous quantification of something as hard-to-define as human culture has important and beneficial applications.&lt;/p&gt;
&lt;p&gt;As for future follow on work. Word vectors are, indeed, very cool. But &lt;a href="https://arxiv.org/abs/1506.06726" target="_blank"&gt;thought vectors&lt;/a&gt; - are even cooler! The ability to quantify individual thoughts could be a boon to humanity when coupled with good visualisation techniques. They could (for example) be used to augment human's understanding of various topics, granting permanence and elucidating what is otherwise ephemeral and complex.&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;p&gt;UPDATE (2023-02-03): This whole blog post is in many ways hopelessly outdated. But in particular I want to make a remark regarding the above "thought vectors": Arguably better terms for this could be "sentence embeddings", "contextualized word embeddings" - the kind of thing you get out of BERT and similar large language models.&lt;/p&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;I wish to thank &lt;a href="https://twitter.com/mattiasostmar" target="_blank"&gt;Mattias Ãstmar&lt;/a&gt; and &lt;a href="https://twitter.com/mikaelhuss" target="_blank"&gt;Mikael Huss&lt;/a&gt;, who provided great insights and feedback throughout the project.&lt;/p&gt;
&lt;h2&gt;&lt;a name="appendix"&gt;&lt;/a&gt;Appendix&lt;/h2&gt;
&lt;p&gt;This analysis has all been carried out in my spare time, so I have not approached it from as many angles I perhaps otherwise would do. I believe it is quite rigorous on the whole, but there are some a few caveats that I feel are important to point out.&lt;/p&gt;
&lt;p&gt;The first is a general point: this method ultimately reflects differences between the &lt;em&gt;corpora&lt;/em&gt; underlying the two sets of word vectors compared. Thus it will only reflect true differences in culture when the corpora are comparable - if I took English wikipedia and compared it against Swedish twitter data, I imagine the results would primarily reflect differences between wikipedia and twitter, rather than Swedish and English. I found it was important to filter word vectors to exclude those that are not true Swedish or English words - otherwise the final results were polluted by gibberish.&lt;/p&gt;
&lt;p&gt;Another caveat is the reliance of the final step on what is ultimately a manual interpretation of the results; I looked at the English-enriched word clusters and offered my interpretation based on what I know about the languages and cultures. There are clearly different ways to interpret the same results. Replication of this technique on other corpus and language pairings could determine how robust these findings are.&lt;/p&gt;
&lt;p&gt;There are also some obvious artefacts in the final results. For example, several Swedish-depleted English word clusters were actually not English words, but were words from another language (Figure 11).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 11: Artefact word clusters - foreign language clusters&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Artefact clusters" src="https://tomwhi.github.com/images/WordVectors/Language_Artefact_clusters.svg"&gt;&lt;/p&gt;
&lt;p&gt;Finally, the least-translatable 72 (10% of all) English word clusters also included some results that seem to reflect culture in some way, which didn't fit into the main results section above. Here they are:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 12: Additional results of interest&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Misc results" src="https://tomwhi.github.com/images/WordVectors/Misc_Or_For_Appendix.svg"&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Social media:&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;If you would like to get in touch, here's my social media:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LinkedIn: &lt;a href="https://www.linkedin.com/in/tomwhitington"&gt;https://www.linkedin.com/in/tomwhitington&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Twitter: &lt;a href="https://twitter.com/Tom_Whitington"&gt;@Tom_Whitington&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mastodon: @tomwhitington@sigmoid.social&lt;/li&gt;
&lt;/ul&gt;</description><guid>https://tomwhi.github.com/posts/using-word-vectors-to-decipher-swedish-culture/</guid><pubDate>Sun, 01 Jan 2017 16:01:43 GMT</pubDate></item></channel></rss>